{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy.stats import itemfreq\n",
    "from statistics import mode\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class decision_tree:\n",
    "    \n",
    "    class node:\n",
    "        def __init__(self, left, right, split_rule, is_leaf, label):\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.split_rule = split_rule\n",
    "            self.is_leaf = is_leaf\n",
    "            self.label = label\n",
    "    \n",
    "    def __init__(self, max_depth=1e10):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def max_count(self, lst):\n",
    "        return max(set(lst), key=list(lst).count)\n",
    "        \n",
    "    # utility function for entropy calculation\n",
    "    def entropy(self, indices):\n",
    "        p = itemfreq(a)[:, 1].astype(float) / len(indices)\n",
    "        return -p.dot(np.log2(p))\n",
    "    \n",
    "    # calculate entropy the number of instances in each class in known\n",
    "    def entropy_n(self, all_n):\n",
    "        p = all_n / (np.sum(all_n)+1e-20)\n",
    "        return -p.dot(np.log2(p+1e-20))\n",
    "    \n",
    "    # calculate the impurity(\"badness\") of the specified split on the input data\n",
    "    def impurity(self, left_label_hist, right_label_hist):\n",
    "        Sl = np.sum(left_label_hist)\n",
    "        Sr = np.sum(right_label_hist)\n",
    "        return (Sl*self.entropy_n(left_label_hist) + Sr * self.entropy_n(right_label_hist)) / (Sl+Sr)\n",
    "    \n",
    "    # find the threshold that best split the data points with a certain feature\n",
    "    # Note: <= th goes to S_left and > th goes to S_right\n",
    "    def find_threshold(self, feature, labels):\n",
    "        all_f = sorted(set(feature)) # sorted in ascending order\n",
    "        all_l = set(labels) # list unique labels\n",
    "        \n",
    "        freq_mat = np.zeros([len(all_f), len(all_l)])\n",
    "        for i, f in enumerate(all_f):\n",
    "            for j, l in enumerate(all_l):\n",
    "                freq_mat[i, j] = len(labels[np.where(labels[np.where(feature==f)]==l)])\n",
    "        \n",
    "        # calculate the average of two neighboring values as threshold\n",
    "        # iterates from min to max\n",
    "        all_threshold = (np.hstack((all_f[1:], all_f[-1])) + all_f) / 2.\n",
    "        \n",
    "        # in the beginning, all goes to the right node\n",
    "        n_left = np.zeros([len(all_l)])\n",
    "        n_right = np.sum(freq_mat, axis=0)\n",
    "        n_left_sum = 0\n",
    "        min_threshold = all_threshold[0]\n",
    "        min_H = self.impurity(n_left, n_right)\n",
    "        # loop through all threshold to find the one with the minimum impurity\n",
    "        for i, th in enumerate(all_threshold):\n",
    "            n_left += freq_mat[i, :]\n",
    "            n_right -= freq_mat[i, :]\n",
    "            H = self.impurity(n_left, n_right)\n",
    "            if H < min_H:\n",
    "                min_H = H\n",
    "                min_threshold = th\n",
    "        return min_threshold, min_H\n",
    "    \n",
    "    \n",
    "    # find the best feature and threshold to split data points\n",
    "    def segmenter(self, data, labels, m=-1):\n",
    "        d = data.shape[1]\n",
    "        if m == -1:\n",
    "            all_features = np.arange(d)\n",
    "        else:\n",
    "            all_features = np.random.choice(range(d), m, replace=False)\n",
    "        min_H = 1e20\n",
    "        min_th = 0\n",
    "        min_i = 0\n",
    "        for i in all_features:\n",
    "            threshold, H = self.find_threshold(data[:, i], labels)\n",
    "            if H < min_H:\n",
    "                min_H = H\n",
    "                min_th = threshold\n",
    "                min_i = i\n",
    "        return min_i, min_th\n",
    "    \n",
    "    # the recurrence function that builds the decision tree\n",
    "    def grow_tree(self, S, depth, m=-1):\n",
    "        if len(set(self.labels[S])) == 1 or depth >= self.max_depth: # pure node or reach maximum depth\n",
    "            return self.node(left=None, right=None, split_rule=None, is_leaf=1, label=self.max_count(self.labels[S]))\n",
    "        else:\n",
    "            min_i, min_th = self.segmenter(self.data[S, :], self.labels[S], m=m)\n",
    "            # the following comprehension might be slow\n",
    "#             Sl = [j for j, x in enumerate(self.data[:, min_i]) if x <= min_th and j in S]\n",
    "#             Sr = [j for j, x in enumerate(self.data[:, min_i]) if x > min_th and j in S]\n",
    "            # update: faster:\n",
    "            Sl = [j for j in S if self.data[j, min_i] <= min_th]\n",
    "            Sr = [j for j in S if self.data[j, min_i] > min_th]\n",
    "            # another: faster:\n",
    "#             Sl = np.intersect1d(np.where(self.data[:, min_i] <= min_th), S)\n",
    "#             Sr = np.intersect1d(np.where(self.data[:, min_i] > min_th), S)\n",
    "            if len(Sl) == 0 or len(Sr) == 0:\n",
    "                return self.node(left=None, right=None, split_rule=None, is_leaf=1, \\\n",
    "                                 label=self.max_count(self.labels[S]))\n",
    "            else:\n",
    "                return self.node(left=self.grow_tree(Sl,depth+1), right=self.grow_tree(Sr,depth+1), split_rule = (min_i, min_th), \\\n",
    "                            is_leaf=0, label=None)\n",
    "        \n",
    "    # train the decision tree\n",
    "    def train(self, data, labels, m=-1):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        S = np.array(range(len(labels)))\n",
    "        self.root = self.grow_tree(S, 1, m=m)\n",
    "        \n",
    "    # predict labels of test data\n",
    "    def predict(self, data, verbose=False):\n",
    "        if data.ndim == 1: # special case of only 1 row (it becomes a 1d vector in numpy)\n",
    "            data = np.reshape(data, [1, len(data)])\n",
    "            N = 1\n",
    "        else:\n",
    "            N = data.shape[0]\n",
    "        labels = np.zeros(N)    \n",
    "            \n",
    "        # predict each data point    \n",
    "        for i in range(N):\n",
    "            d = data[i, :]\n",
    "            current_node = self.root\n",
    "            # going down along the tree\n",
    "            while not current_node.is_leaf: # not reach leaf yet\n",
    "                idx = current_node.split_rule[0]\n",
    "                th = current_node.split_rule[1]\n",
    "                if d[idx] <= th:\n",
    "                    current_node = current_node.left\n",
    "                    if verbose:\n",
    "                        print('Going left')\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "                    if verbose:\n",
    "                        print('Going right')\n",
    "            if verbose:\n",
    "                print()\n",
    "                \n",
    "            labels[i] = current_node.label\n",
    "        return labels\n",
    "\n",
    "    # calculate the prediction accuracy\n",
    "    def accuracy(self, data, true_labels):\n",
    "        labels = self.predict(data, verbose=False)\n",
    "        N = len(labels)\n",
    "        return np.sum(labels == true_labels) / float(N)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class random_forest:\n",
    "    def __init__(self, n_trees=20, n_sample=1000, n_feature=-1, max_depth=1e10):\n",
    "        self.n_trees = n_trees\n",
    "        self.n_sample = n_sample\n",
    "        self.n_feature = -1\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = np.array([decision_tree(max_depth)] * n_trees)\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        if self.n_feature == -1:\n",
    "            d = data.shape[1]\n",
    "            self.n_feature = int(np.sqrt(d)) # num of random features = sqrt(d) is a good guess\n",
    "        for dt in self.trees:\n",
    "            idx = np.random.choice(range(len(data)), self.n_sample)\n",
    "            dt.train(data[idx, :], labels[idx], m=self.n_feature) # activate the random feature mode\n",
    "    \n",
    "    def predict(self, data, verbose=False):\n",
    "        if data.ndim == 1: # special case of only 1 row (it becomes a 1d vector in numpy)\n",
    "            data = np.reshape(data, [1, len(data)])\n",
    "            N = 1\n",
    "        else:\n",
    "            N = data.shape[0]\n",
    "        labels = np.zeros(N)    \n",
    "            \n",
    "        # predict each data point    \n",
    "        for i in range(N):\n",
    "            votes = np.zeros(self.n_trees)\n",
    "            for j, t in enumerate(self.trees):\n",
    "                votes[j] = t.predict(data[i, :], verbose=verbose)\n",
    "            labels[i] = t.max_count(votes)\n",
    "        return labels\n",
    "    \n",
    "    def accuracy(self, data, true_labels):\n",
    "        labels = self.predict(data, verbose=False)\n",
    "        N = len(labels)\n",
    "        return np.sum(labels == true_labels) / float(N)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_row(X):\n",
    "    Xn = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i, :]\n",
    "        Xn[i, :] = (x/(np.sqrt(x.dot(x))+1e-15))\n",
    "    return Xn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23702"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = scipy.io.loadmat('./spam_dist/spam_data.mat')\n",
    "train_X = data['training_data']\n",
    "train_y = data['training_labels'].ravel()\n",
    "test_X = data['test_data']\n",
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18961, 32)\n"
     ]
    }
   ],
   "source": [
    "S = np.random.choice(range(len(train_y)), int(len(train_y)/5*4), replace=False)\n",
    "train_data = train_X[S, :]\n",
    "train_labels = train_y[S]\n",
    "S = np.setdiff1d(range(len(train_y)), S)\n",
    "validation_data = train_X[S, :]\n",
    "validation_labels = train_y[S]\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time:  6.355223894119263\n",
      "Accuracy:  0.788652183084\n"
     ]
    }
   ],
   "source": [
    "dt = decision_tree(15)\n",
    "start = time.time()\n",
    "dt.train(train_data, train_labels)\n",
    "end = time.time()\n",
    "print('Total training time: ', end-start)\n",
    "\n",
    "l = dt.accuracy(validation_data, validation_labels)\n",
    "print('Accuracy: ', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trees=10\n",
    "n_sample=1000\n",
    "n_feature=-1\n",
    "max_depth=30\n",
    "rf = random_forest(n_trees=n_trees, n_sample=n_sample, n_feature=n_feature, max_depth=max_depth)\n",
    "start = time.time()\n",
    "rf.train(train_data, train_labels)\n",
    "end = time.time()\n",
    "print('Total training time: ', end-start)\n",
    "\n",
    "l = rf.accuracy(validation_data, validation_labels)\n",
    "print('Accuracy: ', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82050200379666738"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestCltassifier(n_estimators=30)\n",
    "rfc.fit(train_data, train_labels)\n",
    "rfc.score(validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "SPAM_DIR = 'spam/'\n",
    "HAM_DIR = 'ham/'\n",
    "TEST_DIR = 'test/'\n",
    "NUM_TEST_EXAMPLES = 10000\n",
    "spam_filenames = glob.glob('spam_dist/' + SPAM_DIR + '*.txt')\n",
    "ham_filenames = glob.glob('spam_dist/' + HAM_DIR + '*.txt')\n",
    "test_filenames = ['spam_dist/' + TEST_DIR + str(x) + '.txt' for x in range(NUM_TEST_EXAMPLES)]\n",
    "\n",
    "all_text = []\n",
    "for file in spam_filenames+ham_filenames: # use only training set data to build BOG\n",
    "    with open(file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        all_text.append(f.read())\n",
    "        \n",
    "all_test_text = []\n",
    "for file in test_filenames: # use only training set data to build BOG\n",
    "    with open(file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        all_test_text.append(f.read())\n",
    "        \n",
    "vectorizer = CountVectorizer(min_df=4) # min word length=4\n",
    "train_X = normalize_row(vectorizer.fit_transform(all_text).toarray())\n",
    "test_X = normalize_row(vectorizer.transform(all_test_text).toarray())\n",
    "train_y = np.concatenate((np.ones(len(spam_filenames)), np.zeros(len(ham_filenames))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "std = np.std(train_X, axis=0)\n",
    "# use the max N variance word to build model\n",
    "max_idx = std.argsort()[-200:]\n",
    "train_X_max = train_X[:, max_idx]\n",
    "test_X_max = test_X[:, max_idx]\n",
    "\n",
    "S = np.random.choice(range(len(train_y)), int(len(train_y)/5*4), replace=False)\n",
    "train_data = train_X_max[S, :]\n",
    "train_labels = train_y[S]\n",
    "S = np.setdiff1d(range(len(train_y)), S)\n",
    "validation_data = train_X_max[S, :]\n",
    "validation_labels = train_y[S]\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train random forest for Kaggle submission\n",
    "n_trees=10\n",
    "n_sample=1000\n",
    "n_feature=-1\n",
    "max_depth=30\n",
    "\n",
    "rf = random_forest(n_trees=n_trees, n_sample=n_sample, n_feature=n_feature, max_depth=max_depth)\n",
    "start = time.time()\n",
    "rf.train(train_X_max, train_labels) # use all data\n",
    "end = time.time()\n",
    "print('Total training time: ', end-start)\n",
    "\n",
    "l = rf.accuracy(validation_data, validation_labels)\n",
    "print('Accuracy: ', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y = rf.predict(test_X_max)\n",
    "df = pd.DataFrame({'Category': test_y.astype(int)})\n",
    "df.index.rename('Id', inplace=True)\n",
    "df.to_csv('spam_dist/spam_rf_%d_%d_%d_%d.csv' % (n_trees, n_sample, n_feature, max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./census_dist/train_data.csv')\n",
    "test_data = pd.read_csv('./census_dist/test_data.csv')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./titanic_dist/titanic_training.csv')\n",
    "test_data = pd.read_csv('./titanic_dist/titanic_testing_data.csv')\n",
    "train_data.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
